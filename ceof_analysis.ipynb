{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e2809d",
   "metadata": {},
   "source": [
    "Created by Justin Cooke -- October 2025\n",
    "\n",
    "The purpose of this script is to analyze the first 18-yr cycle of the GOM HYCOM Nature Run (see Dukhovskoy et al. 2015 in Deep-Sea Research I)\n",
    "\n",
    "This notebook will go as follows:\n",
    "    Load HYCOM data from ./hycom_data/*.nc\n",
    "    Calculate eta ref to find deep mesoscale eddies for the Eastern half of the Gulf\n",
    "    Calculate LC Length and Northern Ext over the 18 year period\n",
    "    Conduct a CEOF analysis on the deep eddy dataset\n",
    "    Identify prominent modes and when they peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# Sci computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seawater as sw\n",
    "import scipy.sparse.linalg as sla\n",
    "\n",
    "# Parallel comupting\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# For Data\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grdspc\n",
    "import cmocean as cm\n",
    "\n",
    "# Gen stuff\n",
    "from datetime import date\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using XArray the lat lon and depth data first\n",
    "\n",
    "ds_latlon = xr.open_dataset(\"./hycom_data/hycom_latlon.nc\")\n",
    "ds_depth = xr.open_dataset(\"./hycom_data/hycom_depth.nc\")\n",
    "\n",
    "# Now get the lat, lon, and depth for 22N to 28N and -90W to -83W and depth down to 2000m\n",
    "ds_lon = ds_latlon['Longitude'][:]\n",
    "ds_lat = ds_latlon['Latitude'][:]\n",
    "ds_z = ds_depth['Depth'][:] \n",
    "\n",
    "# finding the index in lon array that corresponds to 90W\n",
    "ind90 = list(np.where(ds_lon >= -90)) \n",
    "ind83 = list(np.where(ds_lon >= -83))\n",
    "nlon90 = ind90[0][0]\n",
    "nlon83 = ind83[0][0] + 1\n",
    "lon = ds_lon[nlon90:nlon83]\n",
    "\n",
    "# finding the indices in lat array that correspond to 22W and 28W to only grab data from this region\n",
    "ind22 = list(np.where(ds_lat >= 22))\n",
    "nlat22 = ind22[0][0] \n",
    "ind28 = list(np.where(ds_lat >= 28))\n",
    "nlat28 = ind28[0][0] + 1\n",
    "lat = ds_lat[nlat22:nlat28]\n",
    "\n",
    "ind2k = list(np.where(ds_z >= 2000))\n",
    "n2k = ind2k[0][0] + 1\n",
    "depth = ds_z[:n2k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9373be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can load the potential temperature and salinity\n",
    "\n",
    "ds_theta = xr.open_dataset(\"./hycom_data/hycom_temp.nc\", chunks={'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "ds_sal = xr.open_dataset(\"./hycom_data/hycom_sal.nc\", chunks={'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "ds_ssh = xr.open_dataset(\"./hycom_data/hycom_ssh.nc\", chunks={'MT': 540, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will mask all points that do not reach 2000 meters depth\n",
    "\n",
    "# Lazily load potential temp and salinity\n",
    "theta = ds_theta['temperature']\n",
    "sal = ds_sal['salinity']\n",
    "\n",
    "# Assign coordinates to the lat, lon, and depth dimensions\n",
    "theta['Longitude'] = lon\n",
    "theta['Latitude'] = lat\n",
    "theta['Depth'] = depth\n",
    "theta['MT'] = ds_theta['MT']\n",
    "\n",
    "sal['Longitude'] = lon\n",
    "sal['Latitude'] = lat\n",
    "sal['Depth'] = depth\n",
    "sal['MT'] = ds_sal['MT']\n",
    "\n",
    "\n",
    "# Find the points that have valid points at each depth over all lat lon\n",
    "depth_mask = theta.notnull().any(dim='MT')\n",
    "\n",
    "# Here we are multiplying our boolean by depth and taking this at the last depth level (the maximum)\n",
    "deepest_valid_depth = (depth_mask * depth).max(dim='Depth')\n",
    "\n",
    "# This is our mask for points that reach at least 2000m\n",
    "has_2000m = deepest_valid_depth >= 2000\n",
    "\n",
    "# Now we are applying our mask\n",
    "masked_theta = theta.where(has_2000m)\n",
    "masked_sal = sal.where(has_2000m)\n",
    "\n",
    "# We are storing the masked potential temperature \n",
    "ds_theta['masked_theta'] = masked_theta\n",
    "ds_sal['masked_salinity'] = masked_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7513ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to downsample and look at a single time instance and depth instance (surface)\n",
    "# This selects the 2d field at time 0 and the surface (depth 0)\n",
    "#slice_2d = ds_theta['masked_theta'].isel(MT=0,Depth=0)\n",
    "\n",
    "# Now this loads in the 2d slice from above\n",
    "#myfield = slice_2d.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914df37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to convert depth to pressure\n",
    "\n",
    "# First we need to define a function dpth which is converts pressure to depth in m\n",
    "\n",
    "def dpth(pres_dbar,lat_deg):\n",
    "    x = np.sin(np.radians(lat_deg))**2\n",
    "    g = 9.780318 * (1.0 + (5.2788e-3 + 2.36e-5 * x) * x)\n",
    "\n",
    "    depth_in_m = (((-1.82e-15 * pres_dbar + 2.279e-10) * pres_dbar - 2.2512e-5) * pres_dbar + 9.72659) * pres_dbar / g\n",
    "\n",
    "    return depth_in_m\n",
    "\n",
    "def prs(depth_meters, latitude_deg, tol=0.001):\n",
    "    # Iteratively compute pressure [dbar] from depth [m] and latitude [deg] \n",
    "\n",
    "    # Parameters\n",
    "    # depth_meters : float or np.ndarray\n",
    "        # Depth in meters\n",
    "    # latitude_deg : float or np.ndarray\n",
    "        # Latitude degrees north (-90 to 90)\n",
    "    # tol          : float, optional\n",
    "\n",
    "    # Returns\n",
    "    # pressure : np.ndarray \n",
    "        # Pressure in decibar (dbar)\n",
    "    # iterations : int\n",
    "        # Number of iterations used to converge\n",
    "\n",
    "    # Convert inputs to arrays\n",
    "\n",
    "    depth_meters = np.atleast_1d(depth_meters).astype(float)\n",
    "    latitude_deg = np.atleast_1d(latitude_deg).astype(float)\n",
    "\n",
    "    # Broadcast latitude to depth shape if needed\n",
    "    if latitude_deg.size == 1:\n",
    "        latitude_deg = np.full_like(depth_meters,latitude_deg)\n",
    "    elif latitude_deg.shape != depth_meters.shape:\n",
    "        if latitude_deg.shape[0] == depth_meters.shape[1]:\n",
    "            latitude_deg = np.tile(latitude_deg, (depth_meters.shape[0],1))\n",
    "        else: \n",
    "            raise ValueError(\"Latitude and Depth must have compatible dimensions\")\n",
    "        \n",
    "    # Initialization\n",
    "    pressure = 1.01 * depth_meters\n",
    "    converged = False\n",
    "    max_iters = 20\n",
    "    iters = 1\n",
    "\n",
    "    while not converged and iters <= max_iters:\n",
    "        d = dpth(pressure,latitude_deg)\n",
    "        new_pressure = pressure + (depth_meters - d) * 1.01\n",
    "        delta = np.abs(new_pressure - pressure)\n",
    "\n",
    "        if np.max(delta) < tol:\n",
    "            converged = True\n",
    "\n",
    "        pressure = new_pressure\n",
    "        iters += 1\n",
    "\n",
    "    if not converged:\n",
    "        pressure[:] = np.nan\n",
    "\n",
    "    return pressure.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting depth to pressure (dbar)\n",
    "\n",
    "# Need to create an mxn array where m = [depth] and n = [latitude] to be used in prs kinda like meshgrid\n",
    "depth_2d, lat_2d = xr.broadcast(depth,lat)\n",
    "\n",
    "# make depth and lat lazy\n",
    "depth_2d = depth_2d.chunk({'Depth': -1, 'Latitude': 50})\n",
    "lat_2d = lat_2d.chunk({'Depth': -1, 'Latitude': 50})\n",
    "\n",
    "# This wrapper (apply_ufunc) allows converting depth to pressure\n",
    "# pressure is the name of our output variable\n",
    "pressure = xr.apply_ufunc( \n",
    "    prs, # the actual function we are calling\n",
    "    depth_2d, # input one which is our mxn depth array\n",
    "    lat_2d, # input two which is our mxn latitude array\n",
    "    input_core_dims=[['Depth', 'Latitude'], ['Depth', 'Latitude']], # input and output explicitly tells xarray both inputs and outputs share the same 2d struct\n",
    "    output_core_dims=[['Depth', 'Latitude']],\n",
    "    dask='parallelized', # executes lazily with Dask\n",
    "    vectorize=True, # ensure pres is applied elementwise across the 2D arrays\n",
    "    output_dtypes=[float], # ensure consistent output type\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True},\n",
    ")\n",
    "\n",
    "# Zero out top row\n",
    "pressure[0,:] = 0.0\n",
    "\n",
    "# Want to create repeating matrices of pressure\n",
    "pressure_full = pressure.expand_dims({\n",
    "    'Longitude': ds_theta['Longitude'],\n",
    "    'MT': ds_theta['MT']\n",
    "}).transpose('MT', 'Depth', 'Latitude', 'Longitude')\n",
    "\n",
    "pressure_full_masked = pressure_full.where(has_2000m)\n",
    "\n",
    "# Ref pressure now\n",
    "pref = xr.zeros_like(pressure_full)\n",
    "pref_masked = pref.where(has_2000m)\n",
    "\n",
    "pressure_full_masked = pressure_full.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "pref_masked = pref_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can handle converting potential temp to temp\n",
    "\n",
    "this_theta = ds_theta['masked_theta']\n",
    "this_sal = ds_sal['masked_salinity']\n",
    "\n",
    "temperature = xr.apply_ufunc(\n",
    "    sw.eos80.temp,\n",
    "    this_sal,\n",
    "    this_theta,\n",
    "    pressure_full_masked,\n",
    "    pref_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*4,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "temp_masked = temperature.where(has_2000m)\n",
    "temp_masked = temp_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "ds_theta['actual_temp_masked'] = temp_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next will be to calculate specific volume anomaly, will have to follow the above I'm sure \n",
    "# Can create constant single value xarrays for s35 and T0 like:\n",
    "\n",
    "s35 = xr.full_like(this_sal, fill_value=35.0)  # e.g., reference salinity\n",
    "T0  = xr.zeros_like(this_theta)  # e.g., reference temp\n",
    "\n",
    "s35_masked = s35.where(has_2000m)\n",
    "T0_masked = T0.where(has_2000m)\n",
    "\n",
    "s35_masked = s35_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "T0_masked  = T0_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43497649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find density using standard salinity and temperature\n",
    "dens_350p = xr.apply_ufunc(\n",
    "    sw.eos80.dens,\n",
    "    s35_masked,\n",
    "    T0_masked,\n",
    "    pressure_full_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*3,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "# Chunk it\n",
    "dens_350p = dens_350p.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "\n",
    "this_temp = ds_theta['actual_temp_masked']\n",
    "this_sal = ds_sal['masked_salinity']\n",
    "\n",
    "# Now find density using salinity and temp\n",
    "dens_stp = xr.apply_ufunc(\n",
    "    sw.eos80.dens,\n",
    "    this_sal,\n",
    "    this_temp,\n",
    "    pressure_full_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*3,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "# Chunk it\n",
    "dens_stp = dens_stp.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate specific volume anomaly\n",
    "\n",
    "sv_350p = 1/dens_350p\n",
    "sv_stp  = 1/dens_stp\n",
    "\n",
    "svan = sv_stp - sv_350p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the tricky part, integrate over the water column to find geopotential anomaly\n",
    "\n",
    "# First, convert from dbar to Pascals\n",
    "p_PA = pressure * 1e4\n",
    "\n",
    "g_real = 9.81\n",
    "\n",
    "# Now we need to do the integration for each latitude slice\n",
    "p_PA_brdcst = p_PA.broadcast_like(svan)\n",
    "\n",
    "gpan = xr.apply_ufunc(\n",
    "    np.trapezoid,\n",
    "    svan,\n",
    "    p_PA_brdcst,\n",
    "    input_core_dims=[['Depth']]*2,\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eta steric and eta ref\n",
    "\n",
    "# Eta Steric first\n",
    "eta_steric = gpan/g_real\n",
    "eta_steric_masked = eta_steric.where(has_2000m)\n",
    "\n",
    "# Now Eta Ref\n",
    "this_ssh = ds_ssh['ssh']\n",
    "this_ssh_masked = this_ssh.where(has_2000m)\n",
    "eta_ref = this_ssh - eta_steric_masked\n",
    "\n",
    "# Remove common mode\n",
    "eta_ref = eta_ref - xr.DataArray.mean(eta_ref,dim=[\"Latitude\",\"Longitude\"],skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "132b2964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########################              ] | 66% Completed | 53m 49ss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m slice2d = eta_ref.isel(MT=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     this_eta_field = \u001b[43mslice2d\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ceofs/lib/python3.12/site-packages/xarray/core/dataarray.py:1241\u001b[39m, in \u001b[36mDataArray.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Trigger loading data into memory and return a new dataarray.\u001b[39;00m\n\u001b[32m   1212\u001b[39m \n\u001b[32m   1213\u001b[39m \u001b[33;03mData will be computed and/or loaded from disk or a remote source.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03mVariable.compute\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1240\u001b[39m new = \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ceofs/lib/python3.12/site-packages/xarray/core/dataarray.py:1167\u001b[39m, in \u001b[36mDataArray.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs) -> Self:\n\u001b[32m   1138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Trigger loading data into memory and return this dataarray.\u001b[39;00m\n\u001b[32m   1139\u001b[39m \n\u001b[32m   1140\u001b[39m \u001b[33;03m    Data will be computed and/or loaded from disk or a remote source.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1165\u001b[39m \u001b[33;03m    Variable.load\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     new = \u001b[38;5;28mself\u001b[39m._from_temp_dataset(ds)\n\u001b[32m   1169\u001b[39m     \u001b[38;5;28mself\u001b[39m._variable = new._variable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ceofs/lib/python3.12/site-packages/xarray/core/dataset.py:557\u001b[39m, in \u001b[36mDataset.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m chunkmanager = get_chunked_array_type(*chunked_data.values())\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, Any], ...] = \u001b[43mchunkmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mchunked_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunked_data, evaluated_data, strict=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m.variables[k].data = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ceofs/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:85\u001b[39m, in \u001b[36mDaskManager.compute\u001b[39m\u001b[34m(self, *data, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m, *data: Any, **kwargs: Any\n\u001b[32m     82\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, _DType_co], ...]:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ceofs/lib/python3.12/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "slice2d = eta_ref.isel(MT=0)\n",
    "with ProgressBar():\n",
    "    this_eta_field = slice2d.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f264589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write eta ref to netcdf file so we don't need to repeat all this b.s.\n",
    "#eta_ref.to_netcdf(\"./hycom_data/hycom_etaref.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ceofs",
   "language": "python",
   "name": ".ceofs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
