{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e2809d",
   "metadata": {},
   "source": [
    "Created by Justin Cooke -- October 2025\n",
    "\n",
    "The purpose of this script is to analyze the first 18-yr cycle of the GOM HYCOM Nature Run (see Dukhovskoy et al. 2015 in Deep-Sea Research I)\n",
    "\n",
    "This notebook will go as follows:\n",
    "    Load HYCOM data from ./hycom_data/*.nc\n",
    "    Calculate eta ref to find deep mesoscale eddies for the Eastern half of the Gulf\n",
    "    Calculate LC Length and Northern Ext over the 18 year period\n",
    "    Conduct a CEOF analysis on the deep eddy dataset\n",
    "    Identify prominent modes and when they peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# Sci computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seawater as sw\n",
    "import scipy.sparse.linalg as sla\n",
    "\n",
    "# Parallel comupting\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# For Data\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grdspc\n",
    "import cmocean as cm\n",
    "\n",
    "# Gen stuff\n",
    "from datetime import date\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using XArray the lat lon and depth data first\n",
    "\n",
    "ds_latlon = xr.open_dataset(\"./hycom_data/hycom_latlon.nc\")\n",
    "ds_depth = xr.open_dataset(\"./hycom_data/hycom_depth.nc\")\n",
    "\n",
    "# Now get the lat, lon, and depth for 22N to 28N and -90W to -83W and depth down to 2000m\n",
    "ds_lon = ds_latlon['Longitude'][:]\n",
    "ds_lat = ds_latlon['Latitude'][:]\n",
    "ds_z = ds_depth['Depth'][:] \n",
    "\n",
    "# finding the index in lon array that corresponds to 90W\n",
    "ind90 = list(np.where(ds_lon >= -90)) \n",
    "ind83 = list(np.where(ds_lon >= -83))\n",
    "nlon90 = ind90[0][0]\n",
    "nlon83 = ind83[0][0] + 1\n",
    "lon = ds_lon[nlon90:nlon83]\n",
    "\n",
    "# finding the indices in lat array that correspond to 22W and 28W to only grab data from this region\n",
    "ind22 = list(np.where(ds_lat >= 22))\n",
    "nlat22 = ind22[0][0] \n",
    "ind28 = list(np.where(ds_lat >= 28))\n",
    "nlat28 = ind28[0][0] + 1\n",
    "lat = ds_lat[nlat22:nlat28]\n",
    "\n",
    "ind2k = list(np.where(ds_z >= 2000))\n",
    "n2k = ind2k[0][0] + 1\n",
    "depth = ds_z[:n2k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9373be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can load the potential temperature and salinity\n",
    "\n",
    "ds_theta = xr.open_dataset(\"./hycom_data/hycom_temp.nc\", chunks={'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "ds_sal = xr.open_dataset(\"./hycom_data/hycom_sal.nc\", chunks={'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "ds_ssh = xr.open_dataset(\"./hycom_data/hycom_ssh.nc\", chunks={'MT': 540, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db76e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(ds_sal['salinity'][:,0,100,100],color=\"#204bc4\",label='Salinity')\n",
    "ax.set_ylabel('Salinity [psu]',color='#204bc4')\n",
    "ax.tick_params(axis='y',labelcolor='#204bc4')\n",
    "ax.set_xlabel('Time [Days]')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ds_theta['temperature'][:,0,100,100],color=\"#e68c07\",label='Potential Temp.')\n",
    "ax2.set_ylabel('Potential Temperature [$^\\circ$C]',color='#e68c07')\n",
    "ax2.tick_params(axis='y',labelcolor='#e68c07')\n",
    "\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(ds_ssh['ssh'][:,100,100],color=\"#204bc4\",label='SSH')\n",
    "ax.set_ylabel('SSH [m]')\n",
    "ax.set_xlabel('Time [Days]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "cb = ax.contourf(ds_ssh['ssh'][6200,:,:],levels=np.linspace(-0.3,0.75,7),extend='both')\n",
    "fig.colorbar(cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt,Nz,_,_ = ds_theta['temperature'].shape\n",
    "\n",
    "Nt_1 = 2100\n",
    "#Nt_2 = 4200\n",
    "Nt_2 = 6100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will mask all points that do not reach 2000 meters depth\n",
    "\n",
    "# Lazily load potential temp and salinity\n",
    "# 0:Nt_1 -- first cycle\n",
    "# Nt_1:Nt_2 -- second cycle\n",
    "# Nt_2: -- final cycle\n",
    "theta = ds_theta['temperature'][Nt_2:,:,:,:]\n",
    "sal = ds_sal['salinity'][Nt_2:,:,:,:]\n",
    "\n",
    "# Assign coordinates to the lat, lon, and depth dimensions\n",
    "theta['Longitude'] = lon\n",
    "theta['Latitude'] = lat\n",
    "theta['Depth'] = depth\n",
    "theta['MT'] = ds_theta['MT'][Nt_2:]\n",
    "\n",
    "sal['Longitude'] = lon\n",
    "sal['Latitude'] = lat\n",
    "sal['Depth'] = depth\n",
    "sal['MT'] = ds_sal['MT'][Nt_2:]\n",
    "\n",
    "\n",
    "# Find the points that have valid points at each depth over all lat lon\n",
    "depth_mask = theta.notnull().any(dim='MT')\n",
    "\n",
    "# Here we are multiplying our boolean by depth and taking this at the last depth level (the maximum)\n",
    "deepest_valid_depth = (depth_mask * depth).max(dim='Depth')\n",
    "\n",
    "# This is our mask for points that reach at least 2000m\n",
    "has_2000m = deepest_valid_depth >= 2000\n",
    "\n",
    "# Now we are applying our mask\n",
    "masked_theta = theta.where(has_2000m)\n",
    "masked_sal = sal.where(has_2000m)\n",
    "\n",
    "# We are storing the masked potential temperature \n",
    "theta['masked'] = masked_theta\n",
    "sal['masked'] = masked_sal\n",
    "\n",
    "# Assign coordinates to the lat, lon, and depth dimensions\n",
    "theta['masked']['Longitude'] = lon\n",
    "theta['masked']['Latitude'] = lat\n",
    "theta['masked']['Depth'] = depth\n",
    "theta['masked']['MT'] = ds_theta['MT'][Nt_2:]\n",
    "\n",
    "sal['masked']['Longitude'] = lon\n",
    "sal['masked']['Latitude'] = lat\n",
    "sal['masked']['Depth'] = depth\n",
    "sal['masked']['MT'] = ds_sal['MT'][Nt_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7513ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to downsample and look at a single time instance and depth instance (surface)\n",
    "# This selects the 2d field at time 0 and the surface (depth 0)\n",
    "#slice_2d = ds_theta['masked_theta'].isel(MT=0,Depth=0)\n",
    "\n",
    "# Now this loads in the 2d slice from above\n",
    "#myfield = slice_2d.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914df37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to convert depth to pressure\n",
    "\n",
    "# First we need to define a function dpth which is converts pressure to depth in m\n",
    "\n",
    "def dpth(pres_dbar,lat_deg):\n",
    "    x = np.sin(np.radians(lat_deg))**2\n",
    "    g = 9.780318 * (1.0 + (5.2788e-3 + 2.36e-5 * x) * x)\n",
    "\n",
    "    depth_in_m = (((-1.82e-15 * pres_dbar + 2.279e-10) * pres_dbar - 2.2512e-5) * pres_dbar + 9.72659) * pres_dbar / g\n",
    "\n",
    "    return depth_in_m\n",
    "\n",
    "def prs(depth_meters, latitude_deg, tol=0.001):\n",
    "    # Iteratively compute pressure [dbar] from depth [m] and latitude [deg] \n",
    "\n",
    "    # Parameters\n",
    "    # depth_meters : float or np.ndarray\n",
    "        # Depth in meters\n",
    "    # latitude_deg : float or np.ndarray\n",
    "        # Latitude degrees north (-90 to 90)\n",
    "    # tol          : float, optional\n",
    "\n",
    "    # Returns\n",
    "    # pressure : np.ndarray \n",
    "        # Pressure in decibar (dbar)\n",
    "    # iterations : int\n",
    "        # Number of iterations used to converge\n",
    "\n",
    "    # Convert inputs to arrays\n",
    "\n",
    "    depth_meters = np.atleast_1d(depth_meters).astype(float)\n",
    "    latitude_deg = np.atleast_1d(latitude_deg).astype(float)\n",
    "\n",
    "    # Broadcast latitude to depth shape if needed\n",
    "    if latitude_deg.size == 1:\n",
    "        latitude_deg = np.full_like(depth_meters,latitude_deg)\n",
    "    elif latitude_deg.shape != depth_meters.shape:\n",
    "        if latitude_deg.shape[0] == depth_meters.shape[1]:\n",
    "            latitude_deg = np.tile(latitude_deg, (depth_meters.shape[0],1))\n",
    "        else: \n",
    "            raise ValueError(\"Latitude and Depth must have compatible dimensions\")\n",
    "        \n",
    "    # Initialization\n",
    "    pressure = 1.01 * depth_meters\n",
    "    converged = False\n",
    "    max_iters = 20\n",
    "    iters = 1\n",
    "\n",
    "    while not converged and iters <= max_iters:\n",
    "        d = dpth(pressure,latitude_deg)\n",
    "        new_pressure = pressure + (depth_meters - d) * 1.01\n",
    "        delta = np.abs(new_pressure - pressure)\n",
    "\n",
    "        if np.max(delta) < tol:\n",
    "            converged = True\n",
    "\n",
    "        pressure = new_pressure\n",
    "        iters += 1\n",
    "\n",
    "    if not converged:\n",
    "        pressure[:] = np.nan\n",
    "\n",
    "    return pressure.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting depth to pressure (dbar)\n",
    "\n",
    "# Need to create an mxn array where m = [depth] and n = [latitude] to be used in prs kinda like meshgrid\n",
    "depth_2d, lat_2d = xr.broadcast(depth,lat)\n",
    "\n",
    "# make depth and lat lazy\n",
    "depth_2d = depth_2d.chunk({'Depth': 13, 'Latitude': 83})\n",
    "lat_2d = lat_2d.chunk({'Depth': 13, 'Latitude': 83})\n",
    "\n",
    "# This wrapper (apply_ufunc) allows converting depth to pressure\n",
    "# pressure is the name of our output variable\n",
    "pressure = xr.apply_ufunc( \n",
    "    prs, # the actual function we are calling\n",
    "    depth_2d, # input one which is our mxn depth array\n",
    "    lat_2d, # input two which is our mxn latitude array\n",
    "    input_core_dims=[['Depth', 'Latitude'], ['Depth', 'Latitude']], # input and output explicitly tells xarray both inputs and outputs share the same 2d struct\n",
    "    output_core_dims=[['Depth', 'Latitude']],\n",
    "    dask='parallelized', # executes lazily with Dask\n",
    "    vectorize=True, # ensure pres is applied elementwise across the 2D arrays\n",
    "    output_dtypes=[float], # ensure consistent output type\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True},\n",
    ")\n",
    "\n",
    "# Zero out top row\n",
    "pressure[0,:] = 0.0\n",
    "\n",
    "# Want to create repeating matrices of pressure\n",
    "pressure_full = pressure.expand_dims({\n",
    "    'Longitude': theta['Longitude'],\n",
    "    'MT': theta['MT']\n",
    "}).transpose('MT', 'Depth', 'Latitude', 'Longitude')\n",
    "\n",
    "pressure_full_masked = pressure_full.where(has_2000m)\n",
    "\n",
    "# Ref pressure now\n",
    "pref = xr.zeros_like(pressure_full)\n",
    "pref_masked = pref.where(has_2000m)\n",
    "\n",
    "pressure_full_masked = pressure_full.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "pref_masked = pref_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can handle converting potential temp to temp\n",
    "\n",
    "this_theta = theta['masked']\n",
    "this_sal = sal['masked']\n",
    "\n",
    "temperature = xr.apply_ufunc(\n",
    "    sw.eos80.temp,\n",
    "    this_sal,\n",
    "    this_theta,\n",
    "    pressure_full_masked,\n",
    "    pref_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*4,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "temp_masked = temperature.where(has_2000m)\n",
    "temp_masked = temp_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "theta['masked_temp'] = temp_masked\n",
    "\n",
    "theta['masked_temp']['Longitude'] = lon\n",
    "theta['masked_temp']['Latitude'] = lat\n",
    "theta['masked_temp']['Depth'] = depth\n",
    "theta['masked_temp']['MT'] = ds_theta['MT'][Nt_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next will be to calculate specific volume anomaly, will have to follow the above I'm sure \n",
    "# Can create constant single value xarrays for s35 and T0 like:\n",
    "\n",
    "s35 = xr.full_like(this_sal, fill_value=35.0)  # e.g., reference salinity\n",
    "T0  = xr.zeros_like(this_theta)  # e.g., reference temp\n",
    "\n",
    "s35_masked = s35.where(has_2000m)\n",
    "T0_masked = T0.where(has_2000m)\n",
    "\n",
    "s35_masked = s35_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "T0_masked  = T0_masked.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43497649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find density using standard salinity and temperature\n",
    "dens_350p = xr.apply_ufunc(\n",
    "    sw.eos80.dens,\n",
    "    s35_masked,\n",
    "    T0_masked,\n",
    "    pressure_full_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*3,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "# Chunk it\n",
    "dens_350p = dens_350p.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})\n",
    "\n",
    "this_temp = theta['masked_temp']\n",
    "this_sal = sal['masked']\n",
    "\n",
    "# Now find density using salinity and temp\n",
    "dens_stp = xr.apply_ufunc(\n",
    "    sw.eos80.dens,\n",
    "    this_sal,\n",
    "    this_temp,\n",
    "    pressure_full_masked,\n",
    "    input_core_dims=[['MT','Depth','Latitude','Longitude']]*3,\n",
    "    output_core_dims=[['MT','Depth','Latitude','Longitude']],\n",
    "    dask='parallelized',\n",
    "    vectorize=True,\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "# Chunk it\n",
    "dens_stp = dens_stp.chunk({'MT': 540, 'Depth': 13, 'Latitude': 83, 'Longitude': 88})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate specific volume anomaly\n",
    "\n",
    "sv_350p = 1/dens_350p\n",
    "sv_stp  = 1/dens_stp\n",
    "\n",
    "svan = sv_stp - sv_350p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the tricky part, integrate over the water column to find geopotential anomaly\n",
    "\n",
    "# First, convert from dbar to Pascals\n",
    "p_PA = pressure * 1e4\n",
    "\n",
    "g_real = 9.81\n",
    "\n",
    "# Now we need to do the integration for each latitude slice\n",
    "p_PA_brdcst = p_PA.broadcast_like(svan)\n",
    "\n",
    "gpan = xr.apply_ufunc(\n",
    "    np.trapezoid,\n",
    "    svan,\n",
    "    p_PA_brdcst,\n",
    "    input_core_dims=[['Depth']]*2,\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eta steric and eta ref\n",
    "\n",
    "# Eta Steric first\n",
    "eta_steric = gpan/g_real\n",
    "eta_steric_masked = eta_steric.where(has_2000m)\n",
    "\n",
    "# Now Eta Ref\n",
    "this_ssh = ds_ssh['ssh'][Nt_2:,:,:]\n",
    "this_ssh_masked = this_ssh.where(has_2000m)\n",
    "eta_ref = this_ssh - eta_steric_masked\n",
    "\n",
    "# Remove common mode\n",
    "eta_ref = eta_ref - xr.DataArray.mean(eta_ref,dim=[\"Latitude\",\"Longitude\"],skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f264589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write eta ref to netcdf file so we don't need to repeat all this b.s.\n",
    "renamed_ = eta_ref.rename('eta_ref')\n",
    "\n",
    "with ProgressBar():\n",
    "    renamed_.to_netcdf(f\"./hycom_data/hycom_etaref_{Nt_2}to6573.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ceofs",
   "language": "python",
   "name": ".ceofs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
